{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Nama : Anisah Mufidah \u00b6 NIM : 170441100087 \u00b6 Program Studi : SISTEM INFORMASI \u00b6","title":"Beranda"},{"location":"#nama-anisah-mufidah","text":"","title":"Nama : Anisah Mufidah"},{"location":"#nim-170441100087","text":"","title":"NIM : 170441100087"},{"location":"#program-studi-sistem-informasi","text":"","title":"Program Studi : SISTEM INFORMASI"},{"location":"decision/","text":"DATAMINING, CLASSIFICATION MENGGUNAKAN DECISION TREE (POHON KEPUTUSAN) \u00b6 Dengan memahami konsep dasar dan matematika di balik pohon keputusan, mari belajar untuk membangun pohon keputusan klasifikasi ! Pohon keputusan adalah model pembelajaran mesin yang diawasi yang digunakan untuk memprediksi target dengan mempelajari aturan keputusan dari fitur. Seperti namanya, kita dapat menganggap model ini sebagai penghancuran data kita dengan membuat keputusan berdasarkan serangkaian pertanyaan. Mari kita perhatikan contoh berikut di mana kita menggunakan pohon keputusan untuk memutuskan suatu kegiatan pada hari tertentu: Berdasarkan fitur dalam set pelatihan , model pohon keputusan mempelajari serangkaian pertanyaan untuk menyimpulkan label kelas dari sampel. Seperti yang bisa kita lihat, pohon keputusan adalah model yang menarik jika kita peduli pada interpretabilitas. A. PENGERTIAN POHON KEPUTUSAN (DECISION TREE) \u00b6 Proses pada pohon keputusan adalah mengubah bentuk data (tabel) menjadi model pohon, mengubah model pohon menjadi rule, dan menyederhanakan rule. Manfaat utama dari penggunaan pohon keputusan adalah kemampuannya untuk membreak down proses pengambilan keputusan yang kompleks menjadi lebih simpel sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Pohon Keputusan juga berguna untuk mengeksplorasi data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target. Pohon keputusan merupakan himpunan aturan IF\u2026THEN. Setiap path dalam tree dihubungkan dengan sebuah aturan, di mana premis terdiri atas sekumpulan node-node yang ditemui, dan kesimpulan dari aturam terdiri atas kelas yang terhubung dengan leaf dari path. Pohon keputusan mempunyai 3 tipe simpul yaitu: 1. Simpul akar, dimana tidak memiliki cabang yang masuk dan memiliki cabang lebih dari satu, terkadang tidak memiliki cabang sama sekali. Simpul ini biasanya berupa atribut yang paling memiliki pengaruh terbesar pada suatu kelas tertentu. 2. Simpul internal, dimana hanya memiliki 1 cabang yang masuk, dan memiliki lebih dari 1 cabang yang keluar. 3. Simpul daun, atau simpul akhir dimana hanya memiliki 1 cabang yang masuk, dan tidak memiliki cabang sama sekali dan menandai bahwa simpul tersebut merupakan label kelas. Tahap awal dilakukan pengujian simpul akar, jika pada pengujian simpul akar menghasilkan sesuatu maka proses pengujian juga dilakukan pada setiap cabang berdasarkan hasil dari pengujian. Hal ini berlaku juga untuk simpul internal dimana suatu kondisi pengujian baru akan diterapkan pada simpul daun. Pada umumnya proses dari sistem pohon keputusan adalah mengadopsi strategi pencarian top-down untuk solusi ruang pencariannya. Pada proses mengklasifikasikan sampel yang tidak diketahui, nilai atribut akan diuji pada pohon keputusan dengan cara melacak jalur dari titik akar sampai titik akhir, kemudian akan diprediksikan kelas yang ditempati sampel baru tersebut. B. Dasar-dasar Pohon Keputusan \u00b6 Pohon keputusan dibangun dengan partisi rekursif - mulai dari simpul akar (dikenal sebagai orang tua pertama), setiap simpul dapat dibagi menjadi simpul anak kiri dan kanan. Node-node ini kemudian dapat dibagi lebih lanjut dan mereka sendiri menjadi node induk dari node anak-anak mereka yang dihasilkan. Sebagai contoh, melihat gambar di atas, simpul akar adalah Pekerjaan yang harus dilakukan? dan membelah menjadi node anak Tetap di dan Outlook berdasarkan apakah ada pekerjaan yang harus dilakukan. Node Outlook selanjutnya terbagi menjadi tiga node anak. Jadi, bagaimana kita tahu apa titik pembelahan optimal pada setiap node? Mulai dari root, data dibagi pada fitur yang menghasilkan Information Gain (IG) terbesar (dijelaskan lebih rinci di bawah). Dalam proses berulang, kami kemudian mengulangi prosedur pemisahan ini pada setiap simpul anak sampai daunnya murni - yaitu sampel di setiap simpul semuanya milik kelas yang sama. Dalam praktiknya, ini dapat menghasilkan pohon yang sangat dalam dengan banyak node, yang dapat dengan mudah menyebabkan overfitting. Jadi, kami biasanya ingin memangkas pohon dengan menetapkan batas untuk kedalaman maksimal pohon. C. Memaksimalkan Information Gain \u00b6 Untuk membagi node pada fitur yang paling informatif, kita perlu mendefinisikan fungsi objektif yang ingin kita optimalkan melalui algoritma tree learning. Di sini, fungsi tujuan kami adalah untuk memaksimalkan perolehan informasi di setiap pemisahan, yang kami definisikan sebagai berikut: Di sini, f adalah fitur untuk melakukan split, Dp, Dleft, dan Dright adalah dataset dari parent dan child node, I adalah ukuran pengotor, Np adalah jumlah total sampel pada node induk, dan Nleft dan Nright adalah jumlah sampel dalam simpul anak. Perhatikan bahwa persamaan di atas adalah untuk pohon keputusan biner - setiap simpul induk dibagi menjadi dua simpul anak saja. Jika kita memiliki pohon keputusan dengan beberapa node, kita cukup menjumlahkan ketidakmurnian semua node. D. POHON KLASIFIKASI Dalam contoh ini, saya akan menggunakan dataset iris klasik. Gunakan kode berikut untuk memuatnya. import sklearn.datasets as datasets import pandas as pd iris = datasets . load_iris () df = pd . DataFrame ( iris . data , columns = iris . feature_names ) y = iris . target Sklearn akan menghasilkan pohon keputusan untuk dataset menggunakan versi yang optimal dari algoritma CART ketika Anda menjalankan kode berikut. from sklearn.tree import DecisionTreeClassifier dtree = DecisionTreeClassifier () dtree . fit ( df , y ) Kita juga dapat mengimpor DecisionTreeRegressor dari sklearn.tree jika kita ingin menggunakan pohon keputusan untuk memprediksi variabel target numerik. Coba alihkan salah satu kolom df dengan variabel y dari atas dan paskan pohon regresi di atasnya. Sekarang kita memiliki pohon keputusan, kita dapat menggunakan paket pydotplus untuk membuat visualisasi untuknya. from sklearn.externals.six import StringIO from IPython.display import Image from sklearn.tree import export_graphviz import pydotplus dot_data = StringIO () export_graphviz ( dtree , out_file = dot_data , filled = True , rounded = True , special_characters = True ) graph = pydotplus . graph_from_dot_data ( dot_data . getvalue ()) Image ( graph . create_png ()) output: Baris \u2018value\u2019 dalam setiap node memberi tahu berapa banyak pengamatan yang diurutkan ke dalam node tersebut yang termasuk dalam masing-masing dari tiga kategori. Kita dapat melihat bahwa fitur X2, yang merupakan panjang kelopak bunga, mampu sepenuhnya membedakan satu spesies bunga (Iris-Setosa) dari yang lain. Kelemahan terbesar untuk pohon keputusan adalah bahwa pemisahan yang dibuatnya pada setiap node akan dioptimalkan untuk dataset yang cocok dengannya. Proses pemisahan ini jarang akan digeneralisasikan dengan baik ke data lain. Namun, dapat menghasilkan sejumlah besar pohon keputusan ini, menyesuaikan dengan cara yang sedikit berbeda, dan menggabungkan prediksi mereka untuk membuat beberapa model terbaik saat ini. semoga bermanfaat, Sekian,terimakasih.. referensi: https://towardsdatascience.com/https-medium-com-lorrli-classification-and-regression-analysis-with-decision-trees-c43cdbc58054 https://medium.com/@rnbrown/creating-and-visualizing-decision-trees-with-python-f8e8fa394176 http://pohonkeputusan.com/konsep-pohon-keputusan-id3-dan-c4-5/?i=1","title":"decision"},{"location":"decision/#datamining-classification-menggunakan-decision-tree-pohon-keputusan","text":"Dengan memahami konsep dasar dan matematika di balik pohon keputusan, mari belajar untuk membangun pohon keputusan klasifikasi ! Pohon keputusan adalah model pembelajaran mesin yang diawasi yang digunakan untuk memprediksi target dengan mempelajari aturan keputusan dari fitur. Seperti namanya, kita dapat menganggap model ini sebagai penghancuran data kita dengan membuat keputusan berdasarkan serangkaian pertanyaan. Mari kita perhatikan contoh berikut di mana kita menggunakan pohon keputusan untuk memutuskan suatu kegiatan pada hari tertentu: Berdasarkan fitur dalam set pelatihan , model pohon keputusan mempelajari serangkaian pertanyaan untuk menyimpulkan label kelas dari sampel. Seperti yang bisa kita lihat, pohon keputusan adalah model yang menarik jika kita peduli pada interpretabilitas.","title":"DATAMINING, CLASSIFICATION MENGGUNAKAN DECISION TREE (POHON KEPUTUSAN)"},{"location":"decision/#a-pengertian-pohon-keputusan-decision-tree","text":"Proses pada pohon keputusan adalah mengubah bentuk data (tabel) menjadi model pohon, mengubah model pohon menjadi rule, dan menyederhanakan rule. Manfaat utama dari penggunaan pohon keputusan adalah kemampuannya untuk membreak down proses pengambilan keputusan yang kompleks menjadi lebih simpel sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Pohon Keputusan juga berguna untuk mengeksplorasi data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target. Pohon keputusan merupakan himpunan aturan IF\u2026THEN. Setiap path dalam tree dihubungkan dengan sebuah aturan, di mana premis terdiri atas sekumpulan node-node yang ditemui, dan kesimpulan dari aturam terdiri atas kelas yang terhubung dengan leaf dari path. Pohon keputusan mempunyai 3 tipe simpul yaitu: 1. Simpul akar, dimana tidak memiliki cabang yang masuk dan memiliki cabang lebih dari satu, terkadang tidak memiliki cabang sama sekali. Simpul ini biasanya berupa atribut yang paling memiliki pengaruh terbesar pada suatu kelas tertentu. 2. Simpul internal, dimana hanya memiliki 1 cabang yang masuk, dan memiliki lebih dari 1 cabang yang keluar. 3. Simpul daun, atau simpul akhir dimana hanya memiliki 1 cabang yang masuk, dan tidak memiliki cabang sama sekali dan menandai bahwa simpul tersebut merupakan label kelas. Tahap awal dilakukan pengujian simpul akar, jika pada pengujian simpul akar menghasilkan sesuatu maka proses pengujian juga dilakukan pada setiap cabang berdasarkan hasil dari pengujian. Hal ini berlaku juga untuk simpul internal dimana suatu kondisi pengujian baru akan diterapkan pada simpul daun. Pada umumnya proses dari sistem pohon keputusan adalah mengadopsi strategi pencarian top-down untuk solusi ruang pencariannya. Pada proses mengklasifikasikan sampel yang tidak diketahui, nilai atribut akan diuji pada pohon keputusan dengan cara melacak jalur dari titik akar sampai titik akhir, kemudian akan diprediksikan kelas yang ditempati sampel baru tersebut.","title":"A. PENGERTIAN POHON KEPUTUSAN (DECISION TREE)"},{"location":"decision/#b-dasar-dasar-pohon-keputusan","text":"Pohon keputusan dibangun dengan partisi rekursif - mulai dari simpul akar (dikenal sebagai orang tua pertama), setiap simpul dapat dibagi menjadi simpul anak kiri dan kanan. Node-node ini kemudian dapat dibagi lebih lanjut dan mereka sendiri menjadi node induk dari node anak-anak mereka yang dihasilkan. Sebagai contoh, melihat gambar di atas, simpul akar adalah Pekerjaan yang harus dilakukan? dan membelah menjadi node anak Tetap di dan Outlook berdasarkan apakah ada pekerjaan yang harus dilakukan. Node Outlook selanjutnya terbagi menjadi tiga node anak. Jadi, bagaimana kita tahu apa titik pembelahan optimal pada setiap node? Mulai dari root, data dibagi pada fitur yang menghasilkan Information Gain (IG) terbesar (dijelaskan lebih rinci di bawah). Dalam proses berulang, kami kemudian mengulangi prosedur pemisahan ini pada setiap simpul anak sampai daunnya murni - yaitu sampel di setiap simpul semuanya milik kelas yang sama. Dalam praktiknya, ini dapat menghasilkan pohon yang sangat dalam dengan banyak node, yang dapat dengan mudah menyebabkan overfitting. Jadi, kami biasanya ingin memangkas pohon dengan menetapkan batas untuk kedalaman maksimal pohon.","title":"B. Dasar-dasar Pohon Keputusan"},{"location":"decision/#c-memaksimalkan-information-gain","text":"Untuk membagi node pada fitur yang paling informatif, kita perlu mendefinisikan fungsi objektif yang ingin kita optimalkan melalui algoritma tree learning. Di sini, fungsi tujuan kami adalah untuk memaksimalkan perolehan informasi di setiap pemisahan, yang kami definisikan sebagai berikut: Di sini, f adalah fitur untuk melakukan split, Dp, Dleft, dan Dright adalah dataset dari parent dan child node, I adalah ukuran pengotor, Np adalah jumlah total sampel pada node induk, dan Nleft dan Nright adalah jumlah sampel dalam simpul anak. Perhatikan bahwa persamaan di atas adalah untuk pohon keputusan biner - setiap simpul induk dibagi menjadi dua simpul anak saja. Jika kita memiliki pohon keputusan dengan beberapa node, kita cukup menjumlahkan ketidakmurnian semua node. D. POHON KLASIFIKASI Dalam contoh ini, saya akan menggunakan dataset iris klasik. Gunakan kode berikut untuk memuatnya. import sklearn.datasets as datasets import pandas as pd iris = datasets . load_iris () df = pd . DataFrame ( iris . data , columns = iris . feature_names ) y = iris . target Sklearn akan menghasilkan pohon keputusan untuk dataset menggunakan versi yang optimal dari algoritma CART ketika Anda menjalankan kode berikut. from sklearn.tree import DecisionTreeClassifier dtree = DecisionTreeClassifier () dtree . fit ( df , y ) Kita juga dapat mengimpor DecisionTreeRegressor dari sklearn.tree jika kita ingin menggunakan pohon keputusan untuk memprediksi variabel target numerik. Coba alihkan salah satu kolom df dengan variabel y dari atas dan paskan pohon regresi di atasnya. Sekarang kita memiliki pohon keputusan, kita dapat menggunakan paket pydotplus untuk membuat visualisasi untuknya. from sklearn.externals.six import StringIO from IPython.display import Image from sklearn.tree import export_graphviz import pydotplus dot_data = StringIO () export_graphviz ( dtree , out_file = dot_data , filled = True , rounded = True , special_characters = True ) graph = pydotplus . graph_from_dot_data ( dot_data . getvalue ()) Image ( graph . create_png ()) output: Baris \u2018value\u2019 dalam setiap node memberi tahu berapa banyak pengamatan yang diurutkan ke dalam node tersebut yang termasuk dalam masing-masing dari tiga kategori. Kita dapat melihat bahwa fitur X2, yang merupakan panjang kelopak bunga, mampu sepenuhnya membedakan satu spesies bunga (Iris-Setosa) dari yang lain. Kelemahan terbesar untuk pohon keputusan adalah bahwa pemisahan yang dibuatnya pada setiap node akan dioptimalkan untuk dataset yang cocok dengannya. Proses pemisahan ini jarang akan digeneralisasikan dengan baik ke data lain. Namun, dapat menghasilkan sejumlah besar pohon keputusan ini, menyesuaikan dengan cara yang sedikit berbeda, dan menggabungkan prediksi mereka untuk membuat beberapa model terbaik saat ini. semoga bermanfaat, Sekian,terimakasih.. referensi: https://towardsdatascience.com/https-medium-com-lorrli-classification-and-regression-analysis-with-decision-trees-c43cdbc58054 https://medium.com/@rnbrown/creating-and-visualizing-decision-trees-with-python-f8e8fa394176 http://pohonkeputusan.com/konsep-pohon-keputusan-id3-dan-c4-5/?i=1","title":"C. Memaksimalkan Information Gain"},{"location":"knn/","text":"DATAMINING , Algoritma Nearest Neighbor (K-Nearest Neighbor/K-NN) \u00b6 A. Pengertian dan Cara Kerja Algoritma K-Nearest Neighbors (KNN) \u00b6 K-nearest neighbors atau knn adalah algoritma yang berfungsi untuk melakukan klasifikasi suatu data berdasarkan data pembelajaran (train data sets), yang diambil dari k tetangga terdekatnya (nearest neighbors). Dengan k merupakan banyaknya tetangga terdekat. Cara Kerja Algoritma K-Nearest Neighbors (KNN) \u00b6 K-nearest neighbors melakukan klasifikasi dengan proyeksi data pembelajaran pada ruang berdimensi banyak. Ruang ini dibagi menjadi bagian-bagian yang merepresentasikan kriteria data pembelajaran. Setiap data pembelajaran direpresentasikan menjadi titik-titik c pada ruang dimensi banyak. model k-NN bekerja dengan mengambil titik data dan melihat titik data berlabel \u2018k\u2019 terdekat. Titik data kemudian diberi label mayoritas poin terdekat \u2018k\u2019. Banyaknya k Tetangga Terdekat \u00b6 Untuk menggunakan algoritma k nearest neighbors, perlu ditentukan banyaknya k tetangga terdekat yang digunakan untuk melakukan klasifikasi data baru. Banyaknya k, sebaiknya merupakan angka ganjil, misalnya k = 1, 2, 3, dan seterusnya. Penentuan nilai k dipertimbangkan berdasarkan banyaknya data yang ada dan ukuran dimensi yang dibentuk oleh data. Semakin banyak data yang ada, angka k yang dipilih sebaiknya semakin rendah. Namun, semakin besar ukuran dimensi data, angka k yang dipilih sebaiknya semakin tinggi. Misalnya, jika K = 3, 2 titik merah dan 1 titik biru,maka titik data yang dimaksud akan diberi label 'merah',karena merah adalah mayoritas. dengan K = 6, 4 titik adalah 'biru' dan 2 adalah 'merah', maka titik data yang dimaksud akan diberi label 'biru', karena 'biru' adalah mayoritas (seperti yang ditunjukkan pada grafik di atas ) Algoritma K-Nearest Neighbors \u00b6 Tentukan k bilangan bulat positif berdasarkan ketersediaan data pembelajaran. Pilih tetangga terdekat dari data baru sebanyak k. Tentukan klasifikasi paling umum pada langkah (ii), dengan menggunakan frekuensi terbanyak. Keluaran klasifikasi dari data sampel baru. B. Contoh implementasi K Nearest Neighbor \u00b6 saya akan menggunakan set data iris yang terkenal untuk contoh KNN . Dataset terdiri dari empat atribut: sepal-width, sepal-length, petal-width dan petal-length. Ini adalah atribut dari jenis spesifik tanaman iris. Tugasnya adalah untuk memprediksi kelas tempat tanaman ini berada. Ada tiga kelas dalam dataset: Iris-setosa, Iris-versicolor dan Iris-virginica. lebih detail dan lengkapnya akan saya bahas di bawah ini import libraries dan dataset import numpy as np import matplotlib.pyplot as plt import pandas as pd url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\" # Tetapkan nama colum ke dataset names = [ 'sepal-length' , 'sepal-width' , 'petal-length' , 'petal-width' , 'Class' ] # Baca dataset ke pandas dataframe dataset = pd . read_csv ( url , names = names ) dataset . head () script di atas akan menampilkan lima baris pertama dari dataset kami seperti yang ditunjukkan di bawah ini: sepal - length sepal - width petal - length petal - width Class 0 5.1 3.5 1.4 0.2 Iris - setosa 1 4.9 3.0 1.4 0.2 Iris - setosa 2 4.7 3.2 1.3 0.2 Iris - setosa 3 4.6 3.1 1.5 0.2 Iris - setosa 4 5.0 3.6 1.4 0.2 Iris - setosa Preprocessing Langkah selanjutnya adalah membagi dataset kami menjadi atribut dan labelnya. Untuk melakukannya, gunakan kode berikut: #membagi atribut(x) dan label(y) X = dataset . iloc [:, : - 1 ] . values y = dataset . iloc [:, 4 ] . values Variabel X berisi empat kolom pertama dari dataset (yaitu atribut) sementara y berisi label. Membagi dataset (training dan testing ) Untuk menghindari pemasangan berlebihan, saya akan membagi dataset menjadi data latih dan data uji. Untuk membuat pelatihan dan menguji pemisahan, jalankan skrip berikut: #membagi data training dan data testing from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.10 ) Script di atas membagi dataset menjadi 90% data training dan 10% data uji. Ini berarti bahwa dari total 150 record, set pelatihan akan berisi 135 record dan set tes berisi 15 record tersebut. Penskalaan fitur Sebelum membuat prediksi aktual, itu selalu merupakan praktik yang baik untuk skala fitur sehingga semuanya dapat dievaluasi secara seragam. Wikipedia menjelaskan alasannya dengan cukup baik: Karena rentang nilai data mentah sangat bervariasi, dalam beberapa algoritma pembelajaran mesin, fungsi objektif tidak akan berfungsi dengan baik tanpa normalisasi. Sebagai contoh, mayoritas pengklasifikasi menghitung jarak antara dua titik dengan jarak Euclidean. Jika salah satu fitur memiliki rentang nilai yang luas, jarak akan diatur oleh fitur khusus ini. Oleh karena itu, rentang semua fitur harus dinormalisasi sehingga setiap fitur berkontribusi sekitar secara proporsional terhadap jarak akhir. Algoritma gradient descent (yang digunakan dalam pelatihan jaringan saraf dan algoritma pembelajaran mesin lainnya) juga lebih cepat berkonvergensi dengan fitur yang dinormalisasi. Script berikut melakukan penskalaan fitur: #penskalaan fitur from sklearn.preprocessing import StandardScaler scaler = StandardScaler () scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) Training and prediction melatih algoritma KNN dan membuat prediksi menggunakan Scikit-Learn. #training and prediction from sklearn.neighbors import KNeighborsClassifier classifier = KNeighborsClassifier ( n_neighbors = 5 ) classifier . fit ( X_train , y_train ) Langkah pertama adalah mengimpor kelas KNeighborsClassifier dari perpustakaan sklearn.neighbors . Di baris kedua, kelas ini diinisialisasi dengan satu parameter, yaitu n_neigbours . Ini pada dasarnya adalah nilai untuk K. Tidak ada nilai ideal untuk K dan dipilih setelah pengujian dan evaluasi, namun untuk memulai, k=5 tampaknya menjadi nilai yang paling umum digunakan untuk algoritma KNN. Langkah terakhir adalah membuat prediksi pada data pengujian. Untuk melakukannya, jalankan skrip berikut: y_pred = classifier . predict ( X_test ) Mengevaluasi Algoritma Untuk mengevaluasi suatu algoritma, confusion_matrix, precision, recall dan skor f1 adalah metrik yang paling umum digunakan. Metode confusion_matrix and classification_report dari sklearn.metrics dapat digunakan untuk menghitung metrik ini. Lihatlah skrip berikut: #mengevaluasi algoritma from sklearn.metrics import classification_report , confusion_matrix print ( confusion_matrix ( y_test , y_pred )) print ( classification_report ( y_test , y_pred )) Output dari skrip di atas terlihat seperti ini: [[ 4 0 0 ] [ 0 5 0 ] [ 0 0 6 ]] precision recall f1 - score support Iris - setosa 1.00 1.00 1.00 4 Iris - versicolor 1.00 1.00 1.00 5 Iris - virginica 1.00 1.00 1.00 6 accuracy 1.00 15 macro avg 1.00 1.00 1.00 15 weighted avg 1.00 1.00 1.00 15 Hasilnya menunjukkan bahwa algoritma KNN tersebut dapat mengklasifikasikan semua 15 record dalam set uji dengan akurasi 100%, yang sangat baik. Meskipun algoritme berkinerja sangat baik dengan dataset ini, tidak selalu hasil yang sama dengan semua aplikasi. Seperti disebutkan sebelumnya, KNN tidak selalu berkinerja baik dengan fitur berdimensi tinggi atau kategorikal. Membandingkan Tingkat Kesalahan dengan Nilai K Di bagian pelatihan dan prediksi, bahwa tidak ada cara untuk mengetahui sebelumnya nilai K yang menghasilkan hasil terbaik di percobaan pertama. saya secara acak memilih 5 sebagai nilai K dan kebetulan menghasilkan akurasi 100%. Salah satu cara untuk membantu menemukan nilai K terbaik adalah dengan memplot grafik nilai K dan tingkat kesalahan yang sesuai untuk dataset. Di bagian ini, saya akan memplot kesalahan rata-rata untuk nilai prediksi set tes untuk semua nilai K antara 1 dan 40. Untuk melakukannya, pertama-tama mari kita menghitung rata-rata kesalahan untuk semua nilai yang diprediksi di mana K berkisar dari 1 dan 40. Jalankan skrip berikut: error = [] # Menghitung kesalahan untuk nilai K antara 1 dan 40 for i in range ( 1 , 40 ): knn = KNeighborsClassifier ( n_neighbors = i ) knn . fit ( X_train , y_train ) pred_i = knn . predict ( X_test ) error . append ( np . mean ( pred_i != y_test )) Skrip di atas mengeksekusi loop dari 1 hingga 40. Dalam setiap iterasi kesalahan rata-rata untuk nilai prediksi set tes dihitung dan hasilnya ditambahkan ke daftar kesalahan. Langkah selanjutnya adalah memplot nilai kesalahan terhadap nilai K. Jalankan skrip berikut untuk membuat plot: plt . figure ( figsize = ( 12 , 6 )) plt . plot ( range ( 1 , 40 ), error , color = 'red' , linestyle = 'dashed' , marker = 'o' , markerfacecolor = 'blue' , markersize = 10 ) plt . title ( 'Error Rate K Value' ) plt . xlabel ( 'K Value' ) plt . ylabel ( 'Mean Error' ) Grafik output terlihat seperti ini: Dari output kita dapat melihat bahwa kesalahan rata-rata adalah nol ketika nilai K adalah 1,3,5 dan antara 7-27 , 29-31. Saya akan menyarankan Anda untuk bermain-main dengan nilai K untuk melihat bagaimana hal itu berdampak pada keakuratan prediksi. referensi: https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/ https://www.advernesia.com/blog/data-science/pengertian-dan-cara-kerja-algoritma-k-nearest-neighbours-knn/ Sekian, terimakasih, semoga bermanfaat","title":"knn"},{"location":"knn/#datamining-algoritma-nearest-neighbor-k-nearest-neighbork-nn","text":"","title":"DATAMINING , Algoritma Nearest Neighbor (K-Nearest Neighbor/K-NN)"},{"location":"knn/#a-pengertian-dan-cara-kerja-algoritma-k-nearest-neighbors-knn","text":"K-nearest neighbors atau knn adalah algoritma yang berfungsi untuk melakukan klasifikasi suatu data berdasarkan data pembelajaran (train data sets), yang diambil dari k tetangga terdekatnya (nearest neighbors). Dengan k merupakan banyaknya tetangga terdekat.","title":"A. Pengertian dan Cara Kerja Algoritma K-Nearest Neighbors (KNN)"},{"location":"knn/#cara-kerja-algoritma-k-nearest-neighbors-knn","text":"K-nearest neighbors melakukan klasifikasi dengan proyeksi data pembelajaran pada ruang berdimensi banyak. Ruang ini dibagi menjadi bagian-bagian yang merepresentasikan kriteria data pembelajaran. Setiap data pembelajaran direpresentasikan menjadi titik-titik c pada ruang dimensi banyak. model k-NN bekerja dengan mengambil titik data dan melihat titik data berlabel \u2018k\u2019 terdekat. Titik data kemudian diberi label mayoritas poin terdekat \u2018k\u2019.","title":"Cara Kerja Algoritma K-Nearest Neighbors (KNN)"},{"location":"knn/#banyaknya-k-tetangga-terdekat","text":"Untuk menggunakan algoritma k nearest neighbors, perlu ditentukan banyaknya k tetangga terdekat yang digunakan untuk melakukan klasifikasi data baru. Banyaknya k, sebaiknya merupakan angka ganjil, misalnya k = 1, 2, 3, dan seterusnya. Penentuan nilai k dipertimbangkan berdasarkan banyaknya data yang ada dan ukuran dimensi yang dibentuk oleh data. Semakin banyak data yang ada, angka k yang dipilih sebaiknya semakin rendah. Namun, semakin besar ukuran dimensi data, angka k yang dipilih sebaiknya semakin tinggi. Misalnya, jika K = 3, 2 titik merah dan 1 titik biru,maka titik data yang dimaksud akan diberi label 'merah',karena merah adalah mayoritas. dengan K = 6, 4 titik adalah 'biru' dan 2 adalah 'merah', maka titik data yang dimaksud akan diberi label 'biru', karena 'biru' adalah mayoritas (seperti yang ditunjukkan pada grafik di atas )","title":"Banyaknya k Tetangga Terdekat"},{"location":"knn/#algoritma-k-nearest-neighbors","text":"Tentukan k bilangan bulat positif berdasarkan ketersediaan data pembelajaran. Pilih tetangga terdekat dari data baru sebanyak k. Tentukan klasifikasi paling umum pada langkah (ii), dengan menggunakan frekuensi terbanyak. Keluaran klasifikasi dari data sampel baru.","title":"Algoritma K-Nearest Neighbors"},{"location":"knn/#b-contoh-implementasi-k-nearest-neighbor","text":"saya akan menggunakan set data iris yang terkenal untuk contoh KNN . Dataset terdiri dari empat atribut: sepal-width, sepal-length, petal-width dan petal-length. Ini adalah atribut dari jenis spesifik tanaman iris. Tugasnya adalah untuk memprediksi kelas tempat tanaman ini berada. Ada tiga kelas dalam dataset: Iris-setosa, Iris-versicolor dan Iris-virginica. lebih detail dan lengkapnya akan saya bahas di bawah ini import libraries dan dataset import numpy as np import matplotlib.pyplot as plt import pandas as pd url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\" # Tetapkan nama colum ke dataset names = [ 'sepal-length' , 'sepal-width' , 'petal-length' , 'petal-width' , 'Class' ] # Baca dataset ke pandas dataframe dataset = pd . read_csv ( url , names = names ) dataset . head () script di atas akan menampilkan lima baris pertama dari dataset kami seperti yang ditunjukkan di bawah ini: sepal - length sepal - width petal - length petal - width Class 0 5.1 3.5 1.4 0.2 Iris - setosa 1 4.9 3.0 1.4 0.2 Iris - setosa 2 4.7 3.2 1.3 0.2 Iris - setosa 3 4.6 3.1 1.5 0.2 Iris - setosa 4 5.0 3.6 1.4 0.2 Iris - setosa Preprocessing Langkah selanjutnya adalah membagi dataset kami menjadi atribut dan labelnya. Untuk melakukannya, gunakan kode berikut: #membagi atribut(x) dan label(y) X = dataset . iloc [:, : - 1 ] . values y = dataset . iloc [:, 4 ] . values Variabel X berisi empat kolom pertama dari dataset (yaitu atribut) sementara y berisi label. Membagi dataset (training dan testing ) Untuk menghindari pemasangan berlebihan, saya akan membagi dataset menjadi data latih dan data uji. Untuk membuat pelatihan dan menguji pemisahan, jalankan skrip berikut: #membagi data training dan data testing from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.10 ) Script di atas membagi dataset menjadi 90% data training dan 10% data uji. Ini berarti bahwa dari total 150 record, set pelatihan akan berisi 135 record dan set tes berisi 15 record tersebut. Penskalaan fitur Sebelum membuat prediksi aktual, itu selalu merupakan praktik yang baik untuk skala fitur sehingga semuanya dapat dievaluasi secara seragam. Wikipedia menjelaskan alasannya dengan cukup baik: Karena rentang nilai data mentah sangat bervariasi, dalam beberapa algoritma pembelajaran mesin, fungsi objektif tidak akan berfungsi dengan baik tanpa normalisasi. Sebagai contoh, mayoritas pengklasifikasi menghitung jarak antara dua titik dengan jarak Euclidean. Jika salah satu fitur memiliki rentang nilai yang luas, jarak akan diatur oleh fitur khusus ini. Oleh karena itu, rentang semua fitur harus dinormalisasi sehingga setiap fitur berkontribusi sekitar secara proporsional terhadap jarak akhir. Algoritma gradient descent (yang digunakan dalam pelatihan jaringan saraf dan algoritma pembelajaran mesin lainnya) juga lebih cepat berkonvergensi dengan fitur yang dinormalisasi. Script berikut melakukan penskalaan fitur: #penskalaan fitur from sklearn.preprocessing import StandardScaler scaler = StandardScaler () scaler . fit ( X_train ) X_train = scaler . transform ( X_train ) X_test = scaler . transform ( X_test ) Training and prediction melatih algoritma KNN dan membuat prediksi menggunakan Scikit-Learn. #training and prediction from sklearn.neighbors import KNeighborsClassifier classifier = KNeighborsClassifier ( n_neighbors = 5 ) classifier . fit ( X_train , y_train ) Langkah pertama adalah mengimpor kelas KNeighborsClassifier dari perpustakaan sklearn.neighbors . Di baris kedua, kelas ini diinisialisasi dengan satu parameter, yaitu n_neigbours . Ini pada dasarnya adalah nilai untuk K. Tidak ada nilai ideal untuk K dan dipilih setelah pengujian dan evaluasi, namun untuk memulai, k=5 tampaknya menjadi nilai yang paling umum digunakan untuk algoritma KNN. Langkah terakhir adalah membuat prediksi pada data pengujian. Untuk melakukannya, jalankan skrip berikut: y_pred = classifier . predict ( X_test ) Mengevaluasi Algoritma Untuk mengevaluasi suatu algoritma, confusion_matrix, precision, recall dan skor f1 adalah metrik yang paling umum digunakan. Metode confusion_matrix and classification_report dari sklearn.metrics dapat digunakan untuk menghitung metrik ini. Lihatlah skrip berikut: #mengevaluasi algoritma from sklearn.metrics import classification_report , confusion_matrix print ( confusion_matrix ( y_test , y_pred )) print ( classification_report ( y_test , y_pred )) Output dari skrip di atas terlihat seperti ini: [[ 4 0 0 ] [ 0 5 0 ] [ 0 0 6 ]] precision recall f1 - score support Iris - setosa 1.00 1.00 1.00 4 Iris - versicolor 1.00 1.00 1.00 5 Iris - virginica 1.00 1.00 1.00 6 accuracy 1.00 15 macro avg 1.00 1.00 1.00 15 weighted avg 1.00 1.00 1.00 15 Hasilnya menunjukkan bahwa algoritma KNN tersebut dapat mengklasifikasikan semua 15 record dalam set uji dengan akurasi 100%, yang sangat baik. Meskipun algoritme berkinerja sangat baik dengan dataset ini, tidak selalu hasil yang sama dengan semua aplikasi. Seperti disebutkan sebelumnya, KNN tidak selalu berkinerja baik dengan fitur berdimensi tinggi atau kategorikal. Membandingkan Tingkat Kesalahan dengan Nilai K Di bagian pelatihan dan prediksi, bahwa tidak ada cara untuk mengetahui sebelumnya nilai K yang menghasilkan hasil terbaik di percobaan pertama. saya secara acak memilih 5 sebagai nilai K dan kebetulan menghasilkan akurasi 100%. Salah satu cara untuk membantu menemukan nilai K terbaik adalah dengan memplot grafik nilai K dan tingkat kesalahan yang sesuai untuk dataset. Di bagian ini, saya akan memplot kesalahan rata-rata untuk nilai prediksi set tes untuk semua nilai K antara 1 dan 40. Untuk melakukannya, pertama-tama mari kita menghitung rata-rata kesalahan untuk semua nilai yang diprediksi di mana K berkisar dari 1 dan 40. Jalankan skrip berikut: error = [] # Menghitung kesalahan untuk nilai K antara 1 dan 40 for i in range ( 1 , 40 ): knn = KNeighborsClassifier ( n_neighbors = i ) knn . fit ( X_train , y_train ) pred_i = knn . predict ( X_test ) error . append ( np . mean ( pred_i != y_test )) Skrip di atas mengeksekusi loop dari 1 hingga 40. Dalam setiap iterasi kesalahan rata-rata untuk nilai prediksi set tes dihitung dan hasilnya ditambahkan ke daftar kesalahan. Langkah selanjutnya adalah memplot nilai kesalahan terhadap nilai K. Jalankan skrip berikut untuk membuat plot: plt . figure ( figsize = ( 12 , 6 )) plt . plot ( range ( 1 , 40 ), error , color = 'red' , linestyle = 'dashed' , marker = 'o' , markerfacecolor = 'blue' , markersize = 10 ) plt . title ( 'Error Rate K Value' ) plt . xlabel ( 'K Value' ) plt . ylabel ( 'Mean Error' ) Grafik output terlihat seperti ini: Dari output kita dapat melihat bahwa kesalahan rata-rata adalah nol ketika nilai K adalah 1,3,5 dan antara 7-27 , 29-31. Saya akan menyarankan Anda untuk bermain-main dengan nilai K untuk melihat bagaimana hal itu berdampak pada keakuratan prediksi. referensi: https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/ https://www.advernesia.com/blog/data-science/pengertian-dan-cara-kerja-algoritma-k-nearest-neighbours-knn/ Sekian, terimakasih, semoga bermanfaat","title":"B. Contoh implementasi K Nearest Neighbor"}]}